{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('stocktwits_nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "843da69ca2ee4f0b534085b112099b4edb437f1dbfafcc3afae3619b662602df"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: Using supervised classification, prune out all of the newsletter-esque posts that are irrelevant to my goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOADING:\n",
    "    messages = pd.read_parquet(\"data/all_messages.parquet\")\n",
    "else:\n",
    "    WORKING_DIR = '/kaggle/input/'\n",
    "    ticker_dir = WORKING_DIR + 'short-financial-messages/data/'\n",
    "    PATH = ticker_dir\n",
    "    EXT = \"*.csv\"\n",
    "\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "\n",
    "    parse_csv = lambda file: pd.read_csv(file, parse_dates=['created_at'])\n",
    "\n",
    "    messages = pd.concat((parse_csv(f) for f in all_csv_files), ignore_index=True, sort=False)\n",
    "\n",
    "    messages.set_index('id', inplace=True)\n",
    "    messages.index = messages.index.map(str)\n",
    "    messages = messages[~messages.index.duplicated(keep='first')]\n",
    "\n",
    "    filter_urls = lambda text: re.sub(r\"http\\S+\", \"\", str(text))\n",
    "    messages['body'] = messages['body'].apply(filter_urls)\n",
    "\n",
    "    messages[\"sentiment\"] = messages[\"sentiment\"].replace({-1: 0})\n",
    "    messages.to_parquet(\"all_messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = messages[messages['sentiment'] != -69]\n",
    "labeled[\"is_spam\"] = -69 * np.ones(len(labeled), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    no_punc = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    words = nltk.word_tokenize(no_punc)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    stemmed = [lemmatiser.lemmatize(word) for word in filtered_words]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (labeled['body'].str.contains(\"smartoptionsÂ® unusual activity alert\"))\n",
    "d = labeled[mask]\n",
    "labeled.loc[d.index, \"is_spam\"] = 1\n",
    "labeled.loc[d.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}