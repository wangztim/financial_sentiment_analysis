{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:31.515985Z",
     "iopub.status.busy": "2020-12-06T04:25:31.515107Z",
     "iopub.status.idle": "2020-12-06T04:25:42.084859Z",
     "shell.execute_reply": "2020-12-06T04:25:42.085517Z"
    },
    "papermill": {
     "duration": 10.591909,
     "end_time": "2020-12-06T04:25:42.085741",
     "exception": false,
     "start_time": "2020-12-06T04:25:31.493832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig, DistilBertTokenizerFast, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:46.605623Z",
     "iopub.status.busy": "2020-12-06T04:25:46.604805Z",
     "iopub.status.idle": "2020-12-06T04:25:46.607820Z",
     "shell.execute_reply": "2020-12-06T04:25:46.608372Z"
    },
    "papermill": {
     "duration": 0.022892,
     "end_time": "2020-12-06T04:25:46.608532",
     "exception": false,
     "start_time": "2020-12-06T04:25:46.585640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONFIG VARIABLES\n",
    "MAX_TOKEN_LENGTH = 160\n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "LOADING = True # True if loading from storage, False if generating variables from scratch\n",
    "BATCH_SIZE = 56\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 3e-5\n",
    "WORKING_DIR = 'stocktwits_posts/' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:46.649255Z",
     "iopub.status.busy": "2020-12-06T04:25:46.648084Z",
     "iopub.status.idle": "2020-12-06T04:25:53.448595Z",
     "shell.execute_reply": "2020-12-06T04:25:53.447770Z"
    },
    "papermill": {
     "duration": 6.827061,
     "end_time": "2020-12-06T04:25:53.448725",
     "exception": false,
     "start_time": "2020-12-06T04:25:46.621664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOADING:\n",
    "    messages = pd.read_parquet(WORKING_DIR + \"all_messages.parquet\")\n",
    "else:\n",
    "    ticker_dir = WORKING_DIR + 'stocktwits'\n",
    "    PATH = ticker_dir\n",
    "    EXT = \"*.csv\"\n",
    "\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "\n",
    "    parse_csv = lambda file: pd.read_csv(file, parse_dates=['created_at'])\n",
    "\n",
    "    messages = pd.concat((parse_csv(f) for f in all_csv_files), ignore_index=True, sort=False)\n",
    "\n",
    "    messages.set_index('id', inplace=True)\n",
    "    messages.index = messages.index.map(str)\n",
    "    messages = messages[~messages.index.duplicated(keep='first')]\n",
    "\n",
    "    filter_urls = lambda text: re.sub(r\"http\\S+\", \"\", str(text))\n",
    "    messages['body'] = messages['body'].apply(filter_urls)\n",
    "\n",
    "    messages[\"sentiment\"] = messages[\"sentiment\"].replace({-1: 0})\n",
    "    messages.to_parquet(\"all_messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:53.489609Z",
     "iopub.status.busy": "2020-12-06T04:25:53.488798Z",
     "iopub.status.idle": "2020-12-06T04:25:55.393298Z",
     "shell.execute_reply": "2020-12-06T04:25:55.392451Z"
    },
    "papermill": {
     "duration": 1.931236,
     "end_time": "2020-12-06T04:25:55.393463",
     "exception": false,
     "start_time": "2020-12-06T04:25:53.462227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages[\"is_spam\"] = -69 * np.ones(len(messages), dtype=np.int)\n",
    "labeled = messages[messages['sentiment'] != -69]\n",
    "labeled_alt = labeled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = [\n",
    "    \"smartoptionsÂ®\",\n",
    "    \"technical alerts\",\n",
    "    \": available to subscribers\",\n",
    "    \"evolution trading\",\n",
    "    \"trade alerts\",\n",
    "    \"trading community\",\n",
    "    \"trading alerts\",\n",
    "    \"sweepcast.com\",\n",
    "    \"optionpros\",\n",
    "    \"freedomstocks.ca\",\n",
    "    \"thetradexchange\",\n",
    "    \"capotrades\",\n",
    "    \"thetradexchange\",\n",
    "    \"pineapplestocks.com\",\n",
    "    \"alert triggered\",\n",
    "    \"xtradesb\",\n",
    "    \"option-alerts.com\",\n",
    "    \"options alert\"\n",
    "    \"alerts triggered\",\n",
    "    \"assetdash.com\",\n",
    "    \"news digest\"\n",
    "    \"beststocksnowapp.com\",\n",
    "    \"drstoxx.com\",\n",
    "    \"echelon-1.com\",\n",
    "    \"wallstjesus.com\",\n",
    "    \"trendspider.com\",\n",
    "    \"gainers watchlist\",\n",
    "    \"freedom stocks\",\n",
    "    \"#optionstradingpulse\",\n",
    "    \"vwapindicator\",\n",
    "    \"on notifications\",\n",
    "    \"trade ideas\",\n",
    "    \"(delayed)\",\n",
    "    'follow for',\n",
    "    \"ðŸ“ˆðŸš€ symbol:\",\n",
    "    \"delayed]\",\n",
    "    \"[delayed\"\n",
    "    \"today&#39;s biggest market cap\",\n",
    "    \"trade-ideas.com\"\n",
    "]\n",
    "\n",
    "spam_indices = [\n",
    "    \"189934349\",\n",
    "    \"142590793\",\n",
    "    \"185792536\",\n",
    "    \"182362237\",\n",
    "    \"226578494\",\n",
    "    \"174519289\",\n",
    "    \"240723002\",\n",
    "    \"242183678\",\n",
    "    \"248681269\",\n",
    "    \"245656196\",\n",
    "    \"243413941\",\n",
    "    \"239273922\",\n",
    "    \"230980738\",\n",
    "    \"255520798\",\n",
    "    \"158019671\",\n",
    "    \"252711617\",\n",
    "    \"252527668\",\n",
    "    \"247522334\",\n",
    "    \"251021498\",\n",
    "    \"207262771\"\n",
    "]\n",
    "\n",
    "false_negatives = [\n",
    "    \"210916827\",\n",
    "    \"86743375\",\n",
    "    \"216738976\",\n",
    "    \"236216134\",\n",
    "    \"203164333\",\n",
    "    \"180138622\",\n",
    "    \"206200249\",\n",
    "    \"127735161\",\n",
    "    \"218513852\",\n",
    "    \"211814549\",\n",
    "    \"215246245\",\n",
    "    \"251010890\",\n",
    "    \"207338547\",\n",
    "    \"233435151\",\n",
    "    \"240829277\",\n",
    "    \"220170011\",\n",
    "    \"136139256\",\n",
    "    \"219269972\",\n",
    "    \"231359105\",\n",
    "    \"166400184\",\n",
    "    \"246096363\",\n",
    "    \"136017785\",\n",
    "    \"222582653\",\n",
    "    \"247547045\",\n",
    "    \"210906734\",\n",
    "    \"247247993\",\n",
    "    \"201056424\",\n",
    "    \"256665740\",\n",
    "    \"114878188\",\n",
    "    \"241643844\",\n",
    "    \"192309512\",\n",
    "    \"86743375\",\n",
    "    \"173490639\",\n",
    "    \"210916827\",\n",
    "    \"173353164\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:55.436213Z",
     "iopub.status.busy": "2020-12-06T04:25:55.435278Z",
     "iopub.status.idle": "2020-12-06T04:25:58.073914Z",
     "shell.execute_reply": "2020-12-06T04:25:58.073016Z"
    },
    "papermill": {
     "duration": 2.66513,
     "end_time": "2020-12-06T04:25:58.074058",
     "exception": false,
     "start_time": "2020-12-06T04:25:55.408928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "vocab_set = set()\n",
    "for symbols in labeled['symbols']:\n",
    "    if symbols is not None and len(symbols) > 0:\n",
    "        for w in symbols:\n",
    "            vocab_set.add(w)\n",
    "tokenizer.add_tokens(list(vocab_set))\n",
    "    \n",
    "def tokenize(input_strings):\n",
    "    return tokenizer(\n",
    "        input_strings, \n",
    "        max_length=MAX_TOKEN_LENGTH, \n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"', \n",
    "        truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It seems that FIQA and FPB data serves as good enough for nonspams, not going to pick out 1000 examples by hand\n",
    "good_indices = labeled.iloc[:4242].index.union(pd.Index(false_negatives))\n",
    "labeled.loc[good_indices, \"is_spam\"] = 0\n",
    "\n",
    "spams = labeled[\"body\"].str.contains('|'.join(spam_words), regex=True)\n",
    "bad_indices = spams[spams == True].index.union(pd.Index(spam_indices))\n",
    "labeled.loc[bad_indices, \"is_spam\"] = 1\n",
    "\n",
    "all_indices = good_indices.union(bad_indices)\n",
    "\n",
    "dataset = labeled.loc[all_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StockTwitsDataset(Dataset):\n",
    "    def __init__(self, messages, labels):\n",
    "        self.encodings = tokenize(messages.tolist())\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labels = self.labels.iloc[idx]\n",
    "        input_ids = self.encodings[\"input_ids\"]\n",
    "        attention_masks = self.encodings[\"attention_mask\"]\n",
    "\n",
    "        res = {\n",
    "            'input_ids': input_ids[idx],\n",
    "            'attention_mask': attention_masks[idx],\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset, dataset[\"is_spam\"], shuffle=True)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, x_test[\"is_spam\"], shuffle=True)\n",
    "\n",
    "train_set = StockTwitsDataset(x_train['body'], y_train)\n",
    "test_set = StockTwitsDataset(x_test['body'], y_test)\n",
    "val_set = StockTwitsDataset(x_val['body'], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "config = DistilBertConfig(num_labels=2, return_dict=True)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=3e-5)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optim,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    total_train_loss = 0\n",
    "    batches_trained = 0\n",
    "    \n",
    "    model = model.train()\n",
    "    for batch in train_loader:        \n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        train_loss = outputs.loss\n",
    "        total_train_loss += train_loss\n",
    "        batches_trained += 1\n",
    "        train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "    print(f'Train loss: {total_train_loss / batches_trained}')\n",
    "\n",
    "    total_val_loss = 0\n",
    "    batches_valed = 0\n",
    "        \n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss = outputs.loss\n",
    "            total_val_loss += val_loss\n",
    "            batches_valed += 1\n",
    "    print(f'Validation loss: {total_val_loss / batches_valed}')\n",
    "            \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_val_loss = 0\n",
    "batches_valed = 0\n",
    "\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        val_loss = outputs.loss\n",
    "        total_val_loss += val_loss\n",
    "        batches_valed += 1\n",
    "print(f'Validation loss: {total_val_loss / batches_valed}')\n",
    "        \n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('stocktwits_nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "843da69ca2ee4f0b534085b112099b4edb437f1dbfafcc3afae3619b662602df"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "papermill": {
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-06T04:25:25.944318",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}