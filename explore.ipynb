{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:31.515985Z",
     "iopub.status.busy": "2020-12-06T04:25:31.515107Z",
     "iopub.status.idle": "2020-12-06T04:25:42.084859Z",
     "shell.execute_reply": "2020-12-06T04:25:42.085517Z"
    },
    "papermill": {
     "duration": 10.591909,
     "end_time": "2020-12-06T04:25:42.085741",
     "exception": false,
     "start_time": "2020-12-06T04:25:31.493832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification, DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:42.172292Z",
     "iopub.status.busy": "2020-12-06T04:25:42.130372Z",
     "iopub.status.idle": "2020-12-06T04:25:46.571710Z",
     "shell.execute_reply": "2020-12-06T04:25:46.570945Z"
    },
    "papermill": {
     "duration": 4.473239,
     "end_time": "2020-12-06T04:25:46.571864",
     "exception": false,
     "start_time": "2020-12-06T04:25:42.098625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    \n",
    "# Step 1: Get the credential from the Cloud SDK\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "\n",
    "# Step 2: Set the credentials\n",
    "user_secrets.set_tensorflow_credential(user_credential)\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:46.605623Z",
     "iopub.status.busy": "2020-12-06T04:25:46.604805Z",
     "iopub.status.idle": "2020-12-06T04:25:46.607820Z",
     "shell.execute_reply": "2020-12-06T04:25:46.608372Z"
    },
    "papermill": {
     "duration": 0.022892,
     "end_time": "2020-12-06T04:25:46.608532",
     "exception": false,
     "start_time": "2020-12-06T04:25:46.585640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONFIG VARIABLES\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "LOADING = True # True if loading from storage, False if generating variables from scratch\n",
    "BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
    "MAX_TOKEN_LENGTH = 160\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:46.649255Z",
     "iopub.status.busy": "2020-12-06T04:25:46.648084Z",
     "iopub.status.idle": "2020-12-06T04:25:53.448595Z",
     "shell.execute_reply": "2020-12-06T04:25:53.447770Z"
    },
    "papermill": {
     "duration": 6.827061,
     "end_time": "2020-12-06T04:25:53.448725",
     "exception": false,
     "start_time": "2020-12-06T04:25:46.621664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOADING:\n",
    "    messages = pd.read_parquet(\"/kaggle/input/short-financial-messages/all_messages.parquet\")\n",
    "else:\n",
    "    WORKING_DIR = '/kaggle/input/'\n",
    "    ticker_dir = WORKING_DIR + 'short-financial-messages/data/'\n",
    "    PATH = ticker_dir\n",
    "    EXT = \"*.csv\"\n",
    "\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "\n",
    "    parse_csv = lambda file: pd.read_csv(file, parse_dates=['created_at'])\n",
    "\n",
    "    messages = pd.concat((parse_csv(f) for f in all_csv_files), ignore_index=True, sort=False)\n",
    "\n",
    "    messages.set_index('id', inplace=True)\n",
    "    messages.index = messages.index.map(str)\n",
    "    messages = messages[~messages.index.duplicated(keep='first')]\n",
    "\n",
    "    filter_urls = lambda text: re.sub(r\"http\\S+\", \"\", str(text))\n",
    "    messages['body'] = messages['body'].apply(filter_urls)\n",
    "\n",
    "    messages[\"sentiment\"] = messages[\"sentiment\"].replace({-1: 0})\n",
    "    messages.to_parquet(\"all_messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:53.489609Z",
     "iopub.status.busy": "2020-12-06T04:25:53.488798Z",
     "iopub.status.idle": "2020-12-06T04:25:55.393298Z",
     "shell.execute_reply": "2020-12-06T04:25:55.392451Z"
    },
    "papermill": {
     "duration": 1.931236,
     "end_time": "2020-12-06T04:25:55.393463",
     "exception": false,
     "start_time": "2020-12-06T04:25:53.462227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    418621\n",
       "0    158565\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled = messages[messages['sentiment'] != -69]\n",
    "SAMPLE_SIZE = int(len(messages[messages['sentiment'] == 0]) * 0.8)\n",
    "\n",
    "bull_indices = labeled[labeled['sentiment'] == 1].index\n",
    "random_bull_indices = np.random.choice(bull_indices, SAMPLE_SIZE, replace=False)\n",
    "bull_sample = labeled.loc[random_bull_indices]\n",
    "\n",
    "bear_indices = labeled[labeled['sentiment'] == 0].index\n",
    "random_bear_indices = np.random.choice(bear_indices, SAMPLE_SIZE, replace=True)\n",
    "bear_sample = labeled.loc[random_bear_indices]\n",
    "\n",
    "labeled_training = pd.concat([bull_sample, bear_sample])\n",
    "labeled_test = labeled.drop(random_bull_indices).drop(random_bear_indices)\n",
    "labeled_test, labeled_val = train_test_split(labeled_test, test_size=0.2)\n",
    "len(labeled_training), len(labeled_val), len(labeled_test)\n",
    "labeled['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:55.436213Z",
     "iopub.status.busy": "2020-12-06T04:25:55.435278Z",
     "iopub.status.idle": "2020-12-06T04:25:58.073914Z",
     "shell.execute_reply": "2020-12-06T04:25:58.073016Z"
    },
    "papermill": {
     "duration": 2.66513,
     "end_time": "2020-12-06T04:25:58.074058",
     "exception": false,
     "start_time": "2020-12-06T04:25:55.408928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b181f1477d47fe865ab4e756a52c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if LOADING:\n",
    "#     tokenizer = DistilBertTokenizerFast.from_pretrained(\"/kaggle/input/tokenizer\")\n",
    "# else:\n",
    "#     tokenizer = DistilBertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#     vocab_set = set()\n",
    "#     for symbols in labeled['symbols']:\n",
    "#         if isinstance(symbols, str) and \"[\" in symbols and \"]\" in symbols:\n",
    "#                 l = literal_eval(symbols)\n",
    "#                 for w in l:\n",
    "#                     vocab_set.add(w)\n",
    "#     tokenizer.add_tokens(list(vocab_set))\n",
    "#     tokenizer.save_pretrained(\"tokenizer\")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "vocab_set = set()\n",
    "for symbols in labeled['symbols']:\n",
    "    if symbols is not None and len(symbols) > 0:\n",
    "        for w in symbols:\n",
    "            vocab_set.add(w)\n",
    "tokenizer.add_tokens(list(vocab_set))\n",
    "    \n",
    "def tokenize(input_strings):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        input_strings, \n",
    "        max_length=MAX_TOKEN_LENGTH, \n",
    "        padding=\"max_length\",\n",
    "        return_tensors='tf', \n",
    "        truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T05:26:20.644202Z",
     "iopub.status.busy": "2020-12-06T04:25:58.129692Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2020-12-06T04:25:58.089825",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = tokenize(labeled_training['body'].tolist())\n",
    "x_val = tokenize(labeled_val['body'].tolist())\n",
    "x_test = tokenize(labeled_test['body'].tolist())\n",
    "\n",
    "y_train = labeled_training['sentiment'].values\n",
    "y_val = labeled_val['sentiment'].values\n",
    "y_test = labeled_test['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((dict(x_train), y_train))\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((dict(x_val), y_val))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((dict(x_test), y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " with strategy.scope():\n",
    "    config = DistilBertConfig(num_labels=2, return_dict=True)\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, config=config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, epsilon=1e-08)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_encoding = train_set.tokenize(test_sentence)\n",
    "\n",
    "# test_out = model(input_ids=test_encoding['input_ids'].to(device), attention_mask=test_encoding['attention_mask'].to(device))\n",
    "# F.softmax(test_out.logits, dim=1)\n",
    "# print(torch.argmax(F.softmax(test_out.logits, dim=1), dim=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "papermill": {
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-06T04:25:25.944318",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}