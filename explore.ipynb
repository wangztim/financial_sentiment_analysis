{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:31.515985Z",
     "iopub.status.busy": "2020-12-06T04:25:31.515107Z",
     "iopub.status.idle": "2020-12-06T04:25:42.084859Z",
     "shell.execute_reply": "2020-12-06T04:25:42.085517Z"
    },
    "papermill": {
     "duration": 10.591909,
     "end_time": "2020-12-06T04:25:42.085741",
     "exception": false,
     "start_time": "2020-12-06T04:25:31.493832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig, DistilBertTokenizerFast, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:46.605623Z",
     "iopub.status.busy": "2020-12-06T04:25:46.604805Z",
     "iopub.status.idle": "2020-12-06T04:25:46.607820Z",
     "shell.execute_reply": "2020-12-06T04:25:46.608372Z"
    },
    "papermill": {
     "duration": 0.022892,
     "end_time": "2020-12-06T04:25:46.608532",
     "exception": false,
     "start_time": "2020-12-06T04:25:46.585640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONFIG VARIABLES\n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "LOADING = True # True if loading from storage, False if generating variables from scratch\n",
    "BATCH_SIZE = 56\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 3e-5\n",
    "WORKING_DIR = './' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:46.649255Z",
     "iopub.status.busy": "2020-12-06T04:25:46.648084Z",
     "iopub.status.idle": "2020-12-06T04:25:53.448595Z",
     "shell.execute_reply": "2020-12-06T04:25:53.447770Z"
    },
    "papermill": {
     "duration": 6.827061,
     "end_time": "2020-12-06T04:25:53.448725",
     "exception": false,
     "start_time": "2020-12-06T04:25:46.621664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOADING:\n",
    "    messages = pd.read_parquet(WORKING_DIR + \"all_messages.parquet\")\n",
    "else:\n",
    "    ticker_dir = WORKING_DIR + 'short-financial-messages/data/stocktwits'\n",
    "    PATH = ticker_dir\n",
    "    EXT = \"*.csv\"\n",
    "\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "\n",
    "    parse_csv = lambda file: pd.read_csv(file, parse_dates=['created_at'])\n",
    "\n",
    "    messages = pd.concat((parse_csv(f) for f in all_csv_files), ignore_index=True, sort=False)\n",
    "\n",
    "    messages.set_index('id', inplace=True)\n",
    "    messages.index = messages.index.map(str)\n",
    "    messages = messages[~messages.index.duplicated(keep='first')]\n",
    "\n",
    "    filter_urls = lambda text: re.sub(r\"http\\S+\", \"\", str(text))\n",
    "    messages['body'] = messages['body'].apply(filter_urls)\n",
    "\n",
    "    messages[\"sentiment\"] = messages[\"sentiment\"].replace({-1: 0})\n",
    "    messages.to_parquet(\"all_messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:53.489609Z",
     "iopub.status.busy": "2020-12-06T04:25:53.488798Z",
     "iopub.status.idle": "2020-12-06T04:25:55.393298Z",
     "shell.execute_reply": "2020-12-06T04:25:55.392451Z"
    },
    "papermill": {
     "duration": 1.931236,
     "end_time": "2020-12-06T04:25:55.393463",
     "exception": false,
     "start_time": "2020-12-06T04:25:53.462227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    418621\n",
       "0    158565\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "labeled = messages[messages['sentiment'] != -69]\n",
    "SAMPLE_SIZE = int(len(messages[messages['sentiment'] == 0]) * 0.8)\n",
    "\n",
    "bull_indices = labeled[labeled['sentiment'] == 1].index\n",
    "random_bull_indices = np.random.choice(bull_indices, SAMPLE_SIZE, replace=False)\n",
    "bull_sample = labeled.loc[random_bull_indices]\n",
    "\n",
    "bear_indices = labeled[labeled['sentiment'] == 0].index\n",
    "random_bear_indices = np.random.choice(bear_indices, SAMPLE_SIZE, replace=True)\n",
    "bear_sample = labeled.loc[random_bear_indices]\n",
    "\n",
    "labeled_training = pd.concat([bull_sample, bear_sample])\n",
    "labeled_test = labeled.drop(random_bull_indices).drop(random_bear_indices)\n",
    "labeled_test, labeled_val = train_test_split(labeled_test, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T04:25:55.436213Z",
     "iopub.status.busy": "2020-12-06T04:25:55.435278Z",
     "iopub.status.idle": "2020-12-06T04:25:58.073914Z",
     "shell.execute_reply": "2020-12-06T04:25:58.073016Z"
    },
    "papermill": {
     "duration": 2.66513,
     "end_time": "2020-12-06T04:25:58.074058",
     "exception": false,
     "start_time": "2020-12-06T04:25:55.408928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "vocab_set = set()\n",
    "for symbols in labeled['symbols']:\n",
    "    if symbols is not None and len(symbols) > 0:\n",
    "        for w in symbols:\n",
    "            vocab_set.add(w)\n",
    "tokenizer.add_tokens(list(vocab_set))\n",
    "    \n",
    "def tokenize(input_strings):\n",
    "    return tokenizer(\n",
    "        input_strings, \n",
    "        max_length=MAX_TOKEN_LENGTH, \n",
    "        padding=\"max_length\",\n",
    "        return_tensors='pt', \n",
    "        truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StockTwitsDataset(Dataset):\n",
    "    MAX_TOKEN_LENGTH = 160\n",
    "\n",
    "    def __init__(self, messages, sentiments):\n",
    "        self.encodings = tokenize(messages.tolist())\n",
    "        self.sentiments = sentiments\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentiment = self.sentiments.iloc[idx]\n",
    "        encoding = self.encodings[idx]\n",
    "\n",
    "        res = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiments': sentiment\n",
    "        }\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() takes 3 positional arguments but 4 were given",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6a07dad6c10f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStockTwitsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStockTwitsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStockTwitsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "train_set = StockTwitsDataset(labeled_training['body'], labeled_training['sentiment'])\n",
    "test_set = StockTwitsDataset(labeled_test['body'], labeled_test['sentiment'])\n",
    "val_set = StockTwitsDataset(labeled_val['body'], labeled_val['sentiment'])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "config = DistilBertConfig(num_labels=2, return_dict=True)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=3e-5)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optim,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    total_train_loss = 0\n",
    "    batches_trained = 0\n",
    "    \n",
    "    model = model.train()\n",
    "    for batch in train_loader:        \n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['sentiments'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        train_loss = outputs.loss\n",
    "        total_train_loss += train_loss\n",
    "        batches_trained += 1\n",
    "        train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "    print(f'Train loss: {total_train_loss / batches_trained}')\n",
    "\n",
    "    total_val_loss = 0\n",
    "    batches_valed = 0\n",
    "        \n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['sentiments'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss = outputs.loss\n",
    "            total_val_loss += val_loss\n",
    "            batches_valed += 1\n",
    "    print(f'Validation loss: {total_val_loss / batches_valed}')\n",
    "            \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('stocktwits_nlp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "843da69ca2ee4f0b534085b112099b4edb437f1dbfafcc3afae3619b662602df"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "papermill": {
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-06T04:25:25.944318",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}